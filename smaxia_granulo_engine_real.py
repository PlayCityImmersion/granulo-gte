# smaxia_granulo_engine_real.py
# =============================================================================
# SMAXIA - MOTEUR GRANULO V4 (R√àGLE SMAXIA: "Comment...")
# =============================================================================
# R√àGLE FONDAMENTALE : Toute QC commence par "Comment" et finit par "?"
# =============================================================================

from __future__ import annotations

import io
import re
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from dataclasses import dataclass
from typing import Dict, List, Optional, Tuple
from collections import Counter, defaultdict
from datetime import datetime
from urllib.parse import urljoin

import requests
import pdfplumber
from bs4 import BeautifulSoup


# =============================================================================
# CONFIGURATION
# =============================================================================
UA = "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36"
REQ_TIMEOUT = 20
MAX_PDF_MB = 30
MIN_QI_CHARS = 25
MAX_WORKERS = 10
EPSILON_PSI = 0.1

SEED_URLS_FRANCE = [
    "https://www.apmep.fr/Annee-2025",
    "https://www.apmep.fr/Annee-2024",
    "https://www.apmep.fr/Annee-2023",
]

_session = None

def get_session():
    global _session
    if _session is None:
        _session = requests.Session()
        _session.headers.update({"User-Agent": UA})
    return _session


# =============================================================================
# TAXONOMIES
# =============================================================================
CHAPTER_KEYWORDS = {
    "SUITES NUM√âRIQUES": {"suite", "suites", "arithm√©tique", "g√©om√©trique", "r√©currence", "limite", "convergence"},
    "FONCTIONS": {"fonction", "d√©riv√©e", "primitive", "int√©grale", "limite", "continuit√©", "asymptote"},
    "PROBABILIT√âS": {"probabilit√©", "al√©atoire", "binomiale", "esp√©rance", "variance", "loi normale"},
    "G√âOM√âTRIE": {"vecteur", "droite", "plan", "espace", "coordonn√©es", "produit scalaire"},
}

QUESTION_VERBS = {"calculer", "d√©terminer", "montrer", "d√©montrer", "justifier", "prouver", "√©tudier", "v√©rifier", "r√©soudre"}

EXCLUDE_WORDS = {"sommaire", "√©dito", "√©ditorial", "bulletin", "revue", "publication", "copyright"}

DELTA_NIVEAU = {"Terminale": 1.0, "Premi√®re": 0.8, "Seconde": 0.6}

COGNITIVE_TRANSFORMS = {
    "calculer": 0.3, "simplifier": 0.25, "factoriser": 0.35,
    "d√©river": 0.4, "int√©grer": 0.45, "r√©soudre": 0.4,
    "d√©montrer": 0.5, "r√©currence": 0.6, "limite": 0.5,
}

MATH_SYMBOL_RE = re.compile(r'[=‚â§‚â•‚â†‚àû‚àë‚à´‚àö‚Üí√ó√∑¬±]|\\frac|\\sum|\d+[,\.]\d+')
SUITE_PATTERN_RE = re.compile(r'\b[uvw]\s*[_\(]\s*n', re.IGNORECASE)
EXERCISE_RE = re.compile(r'\b(?:exercice|question|partie)\s*\d*\b', re.IGNORECASE)


# =============================================================================
# R√àGLE SMAXIA : FORMULATION QC "Comment..."
# =============================================================================

VERBES_CANON = {
    "montrer": "d√©montrer", "d√©montrer": "d√©montrer", "prouver": "d√©montrer",
    "calculer": "calculer", "d√©terminer": "d√©terminer", "trouver": "d√©terminer",
    "√©tudier": "√©tudier", "r√©soudre": "r√©soudre", "exprimer": "exprimer",
}

CONCEPTS_PATTERNS = [
    (r"unique.*solution|solution.*unique|admet.*une.*seule", "l'unicit√© d'une solution"),
    (r"existence.*solution|admet.*solution", "l'existence d'une solution"),
    (r"suite.*g√©om√©trique|g√©om√©trique.*raison", "qu'une suite est g√©om√©trique"),
    (r"suite.*arithm√©tique|arithm√©tique.*raison", "qu'une suite est arithm√©tique"),
    (r"r√©currence|par r√©currence", "une propri√©t√© par r√©currence"),
    (r"limite.*suite|convergence|tend vers.*infini", "la limite d'une suite"),
    (r"d√©riv√©e|variations|croissante|d√©croissante", "les variations d'une fonction"),
    (r"probabilit√©|√©v√©nement", "une probabilit√©"),
    (r"esp√©rance", "une esp√©rance"),
]

def extraire_verbe_principal(texte: str) -> str:
    texte_lower = texte.lower()
    for verbe, canon in VERBES_CANON.items():
        if verbe in texte_lower:
            return canon
    return "traiter"

def extraire_concept_cle(qi_texts: List[str]) -> str:
    combined = " ".join(qi_texts).lower()
    for pattern, concept in CONCEPTS_PATTERNS:
        if re.search(pattern, combined):
            return concept
    return "ce type de probl√®me"

def formuler_titre_qc_smaxia(qi_texts: List[str]) -> str:
    """R√àGLE SMAXIA: QC = 'Comment [VERBE] [CONCEPT] ?'"""
    if not qi_texts:
        return "Comment traiter ce type de probl√®me ?"
    
    verbes = Counter()
    for qi in qi_texts:
        verbes[extraire_verbe_principal(qi)] += 1
    verbe = verbes.most_common(1)[0][0] if verbes else "traiter"
    concept = extraire_concept_cle(qi_texts)
    
    return f"Comment {verbe} {concept} ?"


# =============================================================================
# ARI / FRT / D√âCLENCHEURS PAR CONCEPT
# =============================================================================

ARI_PAR_CONCEPT = {
    "l'unicit√© d'une solution": [
        "1. V√©rifier la continuit√© de f sur l'intervalle",
        "2. √âtudier la monotonie stricte (signe de f')",
        "3. Calculer les images aux bornes f(a) et f(b)",
        "4. V√©rifier que k ‚àà [f(a);f(b)]",
        "5. Appliquer le corollaire du TVI",
        "6. Conclure sur l'unicit√©"
    ],
    "qu'une suite est g√©om√©trique": [
        "1. Exprimer u(n+1)",
        "2. Calculer u(n+1)/u(n)",
        "3. Simplifier l'expression",
        "4. Montrer que le quotient = constante q",
        "5. Conclure"
    ],
    "qu'une suite est arithm√©tique": [
        "1. Exprimer u(n+1)",
        "2. Calculer u(n+1) - u(n)",
        "3. Simplifier",
        "4. Montrer que la diff√©rence = constante r",
        "5. Conclure"
    ],
    "une propri√©t√© par r√©currence": [
        "1. INITIALISATION : V√©rifier P(n‚ÇÄ)",
        "2. H√âR√âDIT√â : Supposer P(n) vraie",
        "3. D√©montrer P(n+1)",
        "4. CONCLUSION par r√©currence"
    ],
    "la limite d'une suite": [
        "1. Identifier la forme de u_n",
        "2. Factoriser par le terme dominant",
        "3. Appliquer limites usuelles",
        "4. Conclure"
    ],
    "les variations d'une fonction": [
        "1. Calculer f'(x)",
        "2. R√©soudre f'(x) = 0",
        "3. Tableau de signes de f'",
        "4. Tableau de variations",
        "5. Extremums"
    ],
}

FRT_PAR_CONCEPT = {
    "l'unicit√© d'une solution": [
        {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Quand l'√©nonc√© demande de montrer qu'une √©quation admet UNE SEULE solution."},
        {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "‚Ä¢ f continue sur [a;b]\n‚Ä¢ f strictement monotone\n‚Ä¢ f(a)=... et f(b)=...\n‚Ä¢ k ‚àà [f(a);f(b)]\n‚Ä¢ Corollaire du TVI ‚Üí unique solution"},
        {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "‚Ä¢ Oublier 'continue'\n‚Ä¢ Oublier 'strictement' monotone\n‚Ä¢ Confondre f(x)=k et f'(x)"},
        {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "L'√©quation admet une unique solution Œ± sur [a;b]."}
    ],
    "qu'une suite est g√©om√©trique": [
        {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Quand l'√©nonc√© demande de prouver qu'une suite est g√©om√©trique."},
        {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "‚Ä¢ Calculer u(n+1)/u(n)\n‚Ä¢ Simplifier\n‚Ä¢ Montrer = constante q"},
        {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "‚Ä¢ V√©rifier u(n) ‚â† 0\n‚Ä¢ Ne pas confondre raison et premier terme"},
        {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "(u_n) est g√©om√©trique de raison q."}
    ],
    "une propri√©t√© par r√©currence": [
        {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Quand l'√©nonc√© demande de d√©montrer 'pour tout n ‚â• n‚ÇÄ'."},
        {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "‚Ä¢ INIT: V√©rifier P(n‚ÇÄ)\n‚Ä¢ H√âR√âDIT√â: Supposer P(n), montrer P(n+1)\n‚Ä¢ CONCLUSION"},
        {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "‚Ä¢ Oublier l'initialisation\n‚Ä¢ Oublier 'supposons P(n) vraie'"},
        {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "Par r√©currence, P(n) vraie pour tout n ‚â• n‚ÇÄ."}
    ],
    "la limite d'une suite": [
        {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Quand l'√©nonc√© demande de calculer une limite."},
        {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "‚Ä¢ Identifier la forme\n‚Ä¢ Factoriser\n‚Ä¢ Appliquer th√©or√®mes"},
        {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "‚Ä¢ Formes ind√©termin√©es\n‚Ä¢ Croissances compar√©es"},
        {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "lim u_n = L (ou ¬±‚àû)."}
    ],
    "les variations d'une fonction": [
        {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Quand l'√©nonc√© demande d'√©tudier les variations."},
        {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "‚Ä¢ Calculer f'\n‚Ä¢ R√©soudre f'=0\n‚Ä¢ Signe de f'\n‚Ä¢ Tableau de variations"},
        {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "‚Ä¢ Erreurs de d√©rivation\n‚Ä¢ Domaine de d√©finition"},
        {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "f croissante sur... d√©croissante sur..."}
    ],
}

DECLENCHEURS_PAR_CONCEPT = {
    "l'unicit√© d'une solution": ["admet une unique solution", "une seule solution", "solution unique"],
    "qu'une suite est g√©om√©trique": ["suite g√©om√©trique", "raison q", "quotient constant"],
    "qu'une suite est arithm√©tique": ["suite arithm√©tique", "raison r", "diff√©rence constante"],
    "une propri√©t√© par r√©currence": ["par r√©currence", "pour tout n", "pour tout entier"],
    "la limite d'une suite": ["limite de la suite", "quand n tend vers", "convergence"],
    "les variations d'une fonction": ["tableau de variations", "signe de f'", "croissante d√©croissante"],
}

ARI_GENERIQUE = ["1. Identifier le probl√®me", "2. Appliquer la m√©thode", "3. Calculer", "4. Conclure"]
FRT_GENERIQUE = [
    {"type": "usage", "title": "üîî 1. QUAND UTILISER", "text": "Identifier les mots-cl√©s de l'√©nonc√©."},
    {"type": "method", "title": "‚úÖ 2. M√âTHODE", "text": "Appliquer la m√©thode appropri√©e."},
    {"type": "trap", "title": "‚ö†Ô∏è 3. PI√àGES", "text": "V√©rifier les conditions."},
    {"type": "conc", "title": "‚úçÔ∏è 4. CONCLUSION", "text": "R√©pondre √† la question."}
]

def generer_ari(concept: str) -> List[str]:
    return ARI_PAR_CONCEPT.get(concept, ARI_GENERIQUE)

def generer_frt(concept: str) -> List[Dict]:
    return FRT_PAR_CONCEPT.get(concept, FRT_GENERIQUE)

def generer_declencheurs(concept: str, qi_texts: List[str]) -> List[str]:
    declencheurs = DECLENCHEURS_PAR_CONCEPT.get(concept, [])[:3]
    # Compl√©ter avec bigrams si n√©cessaire
    if len(declencheurs) < 4 and qi_texts:
        stopwords = {"les", "des", "une", "pour", "que", "qui", "est", "dans", "par", "sur"}
        bigrams = Counter()
        for qi in qi_texts:
            toks = re.findall(r"[a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì]{3,}", qi.lower())
            toks = [t for t in toks if t not in stopwords]
            for i in range(len(toks) - 1):
                bigrams[f"{toks[i]} {toks[i+1]}"] += 1
        for phrase, _ in bigrams.most_common(3):
            if phrase not in declencheurs:
                declencheurs.append(phrase)
    return declencheurs[:5]


# =============================================================================
# OUTILS
# =============================================================================
def normalize_text(text: str) -> str:
    return re.sub(r"\s+", " ", text.lower()).strip()

def tokenize(text: str) -> List[str]:
    return re.findall(r"[a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì]{3,}", normalize_text(text))

def jaccard_similarity(a: List[str], b: List[str]) -> float:
    sa, sb = set(a), set(b)
    if not sa and not sb:
        return 0.0
    return len(sa & sb) / len(sa | sb)

def is_math_content(text: str) -> bool:
    text_lower = text.lower()
    if any(excl in text_lower for excl in EXCLUDE_WORDS):
        return False
    has_verb = any(v in text_lower for v in QUESTION_VERBS)
    has_math = bool(MATH_SYMBOL_RE.search(text)) or bool(SUITE_PATTERN_RE.search(text)) or bool(EXERCISE_RE.search(text))
    return has_verb and has_math


# =============================================================================
# F1 / F2
# =============================================================================
def compute_psi_q(qi_texts: List[str], niveau: str = "Terminale") -> Tuple[float, float]:
    if not qi_texts:
        return EPSILON_PSI, 0
    combined = " ".join(qi_texts).lower()
    sum_tj = sum(w for t, w in COGNITIVE_TRANSFORMS.items() if t in combined)
    delta_c = DELTA_NIVEAU.get(niveau, 1.0)
    psi = min(1.0, (sum_tj + EPSILON_PSI) * delta_c / 3.0)
    return round(psi, 2), round(sum_tj, 2)

def compute_score_f2(n_q: int, n_total: int, t_rec: Optional[float], psi_q: float, alpha: float = 5.0) -> float:
    if n_total == 0:
        return 0.0
    freq_ratio = n_q / n_total
    t_rec_safe = max(0.5, t_rec) if t_rec is not None else 5.0
    recency_factor = 1 + (alpha / t_rec_safe)
    return round(freq_ratio * recency_factor * psi_q * 100, 1)


# =============================================================================
# DATACLASS
# =============================================================================
@dataclass
class QiItem:
    subject_id: str
    subject_file: str
    text: str
    chapter: str = ""
    year: Optional[int] = None


# =============================================================================
# AXIOME SMAXIA : "La QC est la Qi Championne qui a subi une Mue"
# =============================================================================
# PRINCIPE : S√âLECTION + MUTATION (pas g√©n√©ration inventive)
# 0. R√âPARATION (Sanitizer) : D√©coller les mots + nettoyer
# 1. NETTOYAGE : Constantes ‚Üí Invariants
# 2. TRADUCTION : Verbe ‚Üí "Comment + verbe"
# 3. STANDARDISATION : Grammaire + ponctuation
# =============================================================================

class SmaxiaSanitizer:
    """
    STEP 0 : Nettoyage et d√©collage des mots issus de l'extraction PDF.
    Optimis√© pour le fran√ßais math√©matique.
    """
    
    # Dictionnaire de s√©paration fran√ßais math√©matique
    FRENCH_MATH_SPLITS = [
        # Expressions longues (ordre important)
        ('pourtoutentiernaturel', 'pour tout entier naturel '),
        ('pourtoutentier', 'pour tout entier '),
        ('entiernaturel', 'entier naturel '),
        ('uniquesolution', 'unique solution '),
        ('√©quationdiff√©rentielle', '√©quation diff√©rentielle '),
        ('probabilit√©que', 'probabilit√© que '),
        ('probabilit√©de', 'probabilit√© de '),
        ('connexionsoit', 'connexion soit '),
        ('laconnexion', 'la connexion '),
        ('serveurB', 'serveur B '),
        ('est√©gale√†', 'est √©gale √† '),
        ('est√©gal', 'est √©gal '),
        ('soitstable', 'soit stable '),
        ('stableet', 'stable et '),
        ('passeparle', 'passe par le '),
        ('etpasse', 'et passe '),
        
        # Verbes + que/articles
        ('D√©montrerque', 'D√©montrer que '),
        ('d√©montrerque', 'd√©montrer que '),
        ('Montrerque', 'Montrer que '),
        ('montrerque', 'montrer que '),
        ('Prouverque', 'Prouver que '),
        ('prouverque', 'prouver que '),
        ('V√©rifierque', 'V√©rifier que '),
        ('Calculerla', 'Calculer la '),
        ('calculerla', 'calculer la '),
        ('Calculerle', 'Calculer le '),
        ('D√©terminerla', 'D√©terminer la '),
        ('d√©terminerla', 'd√©terminer la '),
        ('√âtudierle', '√âtudier le '),
        ('R√©soudreune', 'R√©soudre une '),
        ('r√©soudreune', 'r√©soudre une '),
        ('End√©duire', 'En d√©duire '),
        ('end√©duire', 'en d√©duire '),
        
        # Articles + noms
        ('quela', 'que la '),
        ('quele', 'que le '),
        ('quel\'', "que l'"),
        ('quepour', 'que pour '),
        ('queune', 'que une '),
        ('quef(', 'que f('),
        ('lalimite', 'la limite '),
        ('lasuite', 'la suite '),
        ('lafonction', 'la fonction '),
        ('laprobabilit√©', 'la probabilit√© '),
        ('lad√©riv√©e', 'la d√©riv√©e '),
        ('lesigne', 'le signe '),
        ('letableau', 'le tableau '),
        ('unesuite', 'une suite '),
        ('unefonction', 'une fonction '),
        ('uneunique', 'une unique '),
        ('une√©quation', 'une √©quation '),
        ('unentier', 'un entier '),
        ('l\'aire', "l'aire "),
        ('l\'√©quation', "l'√©quation "),
        ('l\'intervalle', "l'intervalle "),
        
        # Pr√©positions et connexions
        ('dela', 'de la '),
        ('dele', 'de le '),
        ('deraison', 'de raison '),
        ('dudomaine', 'du domaine '),
        ('surl\'', "sur l'"),
        ('surle', 'sur le '),
        ('sur[', 'sur ['),
        ('dansl\'', "dans l'"),
        ('pourtout', 'pour tout '),
        ('toutentier', 'tout entier '),
        
        # Verbes/adjectifs
        ('suiteest', 'suite est '),
        (')est', ') est '),
        ('estg√©om√©trique', 'est g√©om√©trique '),
        ('estarithm√©tique', 'est arithm√©tique '),
        ('estvraie', 'est vraie '),
        ('estcroissante', 'est croissante '),
        ('estd√©croissante', 'est d√©croissante '),
        ('admetune', 'admet une '),
        ('limitede', 'limite de '),
        ('tendvers', 'tend vers '),
        ('quandn', 'quand n '),
        ('ntend', 'n tend '),
        ('parr√©currence', 'par r√©currence '),
        ('g√©om√©triquede', 'g√©om√©trique de '),
        ('raisonq', 'raison q'),
        ('solutionsur', 'solution sur '),
        ('airedu', 'aire du '),
    ]
    
    def clean_garbage_chars(self, text: str) -> str:
        """Nettoie les r√©sidus d'encodage PDF."""
        text = text.replace("√¢‚Ç¨‚Ñ¢", "'")
        text = text.replace("√Ç", "")
        text = text.replace("\n", " ")
        text = text.replace("\r", " ")
        return re.sub(r'\s+', ' ', text).strip()
    
    def isolate_math_operators(self, text: str) -> str:
        """S√©pare les symboles math√©matiques du texte."""
        # Espace autour des op√©rateurs
        text = re.sub(r'([=<>+])', r' \1 ', text)
        # Espace entre lettre et chiffre
        text = re.sub(r'([a-zA-Z√©√®√™√´√†√¢√π√ª√Æ√Ø√¥√ß])(\d)', r'\1 \2', text)
        # Espace entre chiffre et lettre
        text = re.sub(r'(\d)([a-zA-Z√©√®√™√´√†√¢√π√ª√Æ√Ø√¥√ß])', r'\1 \2', text)
        return text
    
    def repair_glued_french(self, text: str) -> str:
        """R√©pare les mots coll√©s avec le dictionnaire fran√ßais math√©matique."""
        result = text
        
        # Appliquer toutes les s√©parations
        for glued, separated in self.FRENCH_MATH_SPLITS:
            result = result.replace(glued, separated)
        
        # R√®gle g√©n√©rique : espace avant majuscule au milieu d'un mot
        result = re.sub(r'([a-z√©√®√™√´√†√¢√π√ª√Æ√Ø√¥√ß])([A-Z√â√à√ä√ã√Ä√Ç√ô√õ√é√è√î√á])', r'\1 \2', result)
        
        # Espace apr√®s ponctuation
        result = re.sub(r'([,;])([a-zA-Z√©√®√™√´√†√¢√π√ª√Æ√Ø√¥√ß])', r'\1 \2', result)
        
        return result
    
    def sanitize(self, raw_text: str) -> str:
        """
        PIPELINE STEP 0 SMAXIA - Nettoyage complet.
        """
        # 1. Nettoyage basique (encodage)
        clean_1 = self.clean_garbage_chars(raw_text)
        
        # 2. Isolation des op√©rateurs math√©matiques
        clean_2 = self.isolate_math_operators(clean_1)
        
        # 3. D√©collage des mots fran√ßais
        clean_3 = self.repair_glued_french(clean_2)
        
        # 4. Nettoyage final des espaces
        final = re.sub(r'\s+', ' ', clean_3).strip()
        
        return final


# Instance globale du Sanitizer
_sanitizer = SmaxiaSanitizer()

def operation_nettoyage(texte: str) -> str:
    """
    OP√âRATION 1 : NETTOYAGE - Remplacement des constantes par leurs formes INVARIANTES
    
    PRINCIPE : On ne SUPPRIME pas, on REMPLACE par la forme g√©n√©rique
    - 0,7 ‚Üí k
    - [0;6] ‚Üí [a;b]
    - f(t) ‚Üí f(x)
    - la suite (u_n) ‚Üí une suite
    - Œ±, Œ≤ ‚Üí supprim√©s (variables de r√©sultat)
    """
    result = texte
    
    # === REMPLACEMENTS (pas suppressions) ===
    
    # Intervalles num√©riques ‚Üí [a;b] ou [a;+‚àû[
    result = re.sub(r'\[\s*-?\d+[,\.]?\d*\s*[;,]\s*-?\d+[,\.]?\d*\s*\]', '[a;b]', result)
    result = re.sub(r'\[\s*-?\d+[,\.]?\d*\s*[;,]\s*\+?‚àû\s*\[', '[a;+‚àû[', result)
    result = re.sub(r'\]\s*-‚àû\s*[;,]\s*-?\d+[,\.]?\d*\s*\]', ']-‚àû;b]', result)
    result = re.sub(r'\]\s*-‚àû\s*[;,]\s*\+?‚àû\s*\[', ']-‚àû;+‚àû[', result)
    
    # f(x)=0,7 ‚Üí f(x)=k (garder la structure √©quation)
    result = re.sub(r'([fgh])\s*\(\s*[txns]\s*\)\s*=\s*-?\d+[,\.]?\d*', r'\1(x)=k ', result)
    
    # f(t), g(x), h(n) ‚Üí f(x) (variable canonique)
    result = re.sub(r'\b([fgh])\s*\(\s*[txns]\s*\)', r'\1(x)', result)
    
    # Nombres d√©cimaux isol√©s (apr√®s =) ‚Üí k
    result = re.sub(r'=\s*-?\d+[,\.]?\d*', '=k ', result)
    
    # la suite (u_n), la suite (v_n) ‚Üí une suite
    result = re.sub(r'la suite\s*\(\s*[uvw]\s*_?\s*n\s*\)', 'une suite', result)
    result = re.sub(r'la suite\s+[uvw]\s*_?\s*n', 'une suite', result)
    
    # (u_n), (v_n) seul ‚Üí une suite (si pr√©c√©d√© de "de")
    result = re.sub(r'de\s*\(\s*[uvw]\s*_?\s*n\s*\)', 'd\'une suite', result)
    result = re.sub(r'\(\s*[uvw]\s*_?\s*n\s*\)', '', result)
    
    # q=2, r=3 ‚Üí q (raison g√©n√©rique)
    result = re.sub(r'([qr])\s*=\s*-?\d+[,\.]?\d*', r'\1', result)
    
    # Lettres grecques isol√©es (r√©sultats) ‚Üí supprim√©es
    result = re.sub(r'\s+[Œ±Œ≤Œ≥Œ¥]\s+', ' ', result)
    result = re.sub(r'\s+[Œ±Œ≤Œ≥Œ¥]\s*$', '', result)
    result = re.sub(r'\s+[Œ±Œ≤Œ≥Œ¥]\s*sur', ' sur', result)
    
    # P(n) reste P(n) - c'est une propri√©t√© g√©n√©rique
    
    # Ann√©es ‚Üí supprim√©es
    result = re.sub(r'\b20\d{2}\b', '', result)
    
    # Noms propres ‚Üí supprim√©s
    noms_propres = ['jean', 'marie', 'pierre', 'paul', 'alice', 'bob', 'urne', 'd√©']
    for nom in noms_propres:
        result = re.sub(rf'\b{nom}\b', '', result, flags=re.IGNORECASE)
    
    # Nettoyer espaces multiples
    result = re.sub(r'\s+', ' ', result)
    
    # Nettoyer ponctuation orpheline
    result = re.sub(r'\s*[,;]\s*[,;]+', ',', result)
    result = re.sub(r'^\s*[,;\.]\s*', '', result)
    result = re.sub(r'\s*\.\s*$', '', result)
    
    return result.strip()


def operation_traduction(texte: str) -> str:
    """
    OP√âRATION 2 : TRADUCTION - Verbe imp√©ratif ‚Üí Interrogatif m√©thode
    - "D√©montrer que" ‚Üí "Comment d√©montrer que"
    - "Calculer" ‚Üí "Comment calculer"
    """
    result = texte.strip()
    
    # Liste des verbes d'action √† transformer
    verbes = [
        'd√©montrer', 'montrer', 'prouver', '√©tablir',
        'calculer', 'd√©terminer', 'trouver', 'chercher',
        '√©tudier', 'analyser', 'examiner',
        'r√©soudre', 'v√©rifier', 'justifier',
        'exprimer', 'expliciter', 'pr√©ciser',
        'en d√©duire', 'd√©duire', 'conclure'
    ]
    
    # V√©rifier si commence d√©j√† par "Comment"
    if result.lower().startswith('comment'):
        return result
    
    # Chercher le verbe au d√©but et pr√©fixer par "Comment"
    for verbe in verbes:
        pattern = rf'^{verbe}\b'
        if re.match(pattern, result, re.IGNORECASE):
            # Garder la casse du verbe original mais ajouter "Comment"
            return f"Comment {result[0].lower()}{result[1:]}"
    
    # Si aucun verbe trouv√© au d√©but, chercher dans la phrase
    for verbe in verbes:
        if verbe in result.lower():
            return f"Comment {result[0].lower()}{result[1:]}"
    
    # Fallback : ajouter "Comment" quand m√™me
    return f"Comment {result[0].lower()}{result[1:]}"


def operation_standardisation(texte: str) -> str:
    """
    OP√âRATION 3 : STANDARDISATION - Corrections grammaticales + ponctuation
    """
    result = texte
    
    # === √âLISIONS FRAN√áAISES ===
    result = re.sub(r'\bde une\b', "d'une", result)
    result = re.sub(r'\bque une\b', "qu'une", result)
    result = re.sub(r'\bla une\b', "l'une", result)
    result = re.sub(r'\bde un\b', "d'un", result)
    result = re.sub(r'\bque un\b', "qu'un", result)
    result = re.sub(r'\bsi il\b', "s'il", result)
    result = re.sub(r'\bde entier\b', "d'entier", result)
    
    # Nettoyer =k doublons et erreurs
    result = re.sub(r'=k=k', '=k', result)
    result = re.sub(r'q\s*=\s*k', 'q', result)  # raison q, pas q=k
    result = re.sub(r'r\s*=\s*k', 'r', result)  # raison r, pas r=k
    
    # Nettoyer les espaces autour de l'infini
    result = re.sub(r'\+\s*‚àû', '+‚àû', result)
    result = re.sub(r'-\s*‚àû', '-‚àû', result)
    
    # Nettoyer la fin de phrase
    result = re.sub(r'\s*[\.,:;]+\s*$', '', result)
    
    # Ajouter le point d'interrogation final
    if not result.endswith('?'):
        result += ' ?'
    
    return result


def mue_qi_vers_qc(qi_championne: str) -> str:
    """
    ALGORITHME DE MUE : Transforme la Qi Championne en titre QC
    
    Applique les 4 op√©rations dans l'ordre :
    0. SANITIZER : Nettoyer + d√©coller les mots (SmaxiaSanitizer)
    1. NETTOYAGE : Remplacer constantes ‚Üí invariants
    2. TRADUCTION : Verbe ‚Üí "Comment + verbe"
    3. STANDARDISATION : Grammaire + ponctuation
    """
    # √âtape 0 : Sanitizer (nettoyage + d√©collage)
    etape0 = _sanitizer.sanitize(qi_championne)
    
    # √âtape 1 : Nettoyage (constantes ‚Üí invariants)
    etape1 = operation_nettoyage(etape0)
    
    # √âtape 2 : Traduction (verbe ‚Üí "Comment + verbe")
    etape2 = operation_traduction(etape1)
    
    # √âtape 3 : Standardisation (grammaire + ponctuation)
    etape3 = operation_standardisation(etape2)
    
    # Nettoyer et capitaliser
    result = etape3.strip()
    if result:
        result = result[0].upper() + result[1:]
    
    # Limiter la longueur si trop long
    if len(result) > 120:
        result = result[:120].rsplit(' ', 1)[0] + '... ?'
    
    return result


# =============================================================================
# CLUSTERING Qi ‚Üí QC (AXIOME : S√âLECTION + MUE)
# =============================================================================

def compute_qi_representativite(qi_text: str, all_qi_texts: List[str]) -> float:
    """
    Calcule le score de repr√©sentativit√© d'une Qi.
    = Combien de mots-cl√©s de cette Qi sont partag√©s avec les autres Qi du cluster.
    """
    qi_tokens = set(tokenize(qi_text))
    if not qi_tokens:
        return 0.0
    
    # Compter les occurrences de chaque token dans tout le cluster
    all_tokens = Counter()
    for txt in all_qi_texts:
        all_tokens.update(tokenize(txt))
    
    # Score = somme des fr√©quences des tokens de cette Qi
    score = sum(all_tokens.get(t, 0) for t in qi_tokens)
    
    return score


def cluster_qi_to_qc(qis: List[QiItem], sim_threshold: float = 0.25) -> List[Dict]:
    """
    Clustering des Qi en QC selon l'AXIOME SMAXIA.
    
    PROCESSUS :
    1. Regrouper les Qi par similarit√©
    2. Pour chaque cluster, √âLIRE la Qi Championne (meilleure repr√©sentativit√©)
    3. Appliquer la MUE sur la Championne ‚Üí Titre QC
    """
    if not qis:
        return []
    
    ALPHA = 5.0
    total_qi = len(qis)
    
    # √âtape 1: Clustering par similarit√© lexicale
    clusters = []
    for qi in qis:
        toks = tokenize(qi.text)
        if not toks:
            continue
        
        best_i, best_sim = None, 0.0
        for i, c in enumerate(clusters):
            sim = jaccard_similarity(toks, c["rep_tokens"])
            if sim > best_sim:
                best_sim, best_i = sim, i
        
        if best_i is not None and best_sim >= sim_threshold:
            clusters[best_i]["qis"].append(qi)
            clusters[best_i]["rep_tokens"] = list(set(clusters[best_i]["rep_tokens"]) | set(toks))
        else:
            clusters.append({
                "rep_tokens": toks,
                "qis": [qi]
            })
    
    # √âtape 2 & 3: Pour chaque cluster, √©lire la Championne et appliquer la MUE
    qc_out = []
    
    for c in clusters:
        cluster_qis = c["qis"]
        qi_texts = [q.text for q in cluster_qis]
        n_q = len(cluster_qis)
        
        # === √âLECTION : Trouver la Qi Championne ===
        # Score de repr√©sentativit√© = combien cette Qi partage avec les autres
        best_qi = None
        best_representativite = -1
        
        for qi in cluster_qis:
            rep_score = compute_qi_representativite(qi.text, qi_texts)
            if rep_score > best_representativite:
                best_representativite = rep_score
                best_qi = qi
        
        if not best_qi:
            best_qi = cluster_qis[0]
        
        # === MUE : Transformer la Championne en QC ===
        titre = mue_qi_vers_qc(best_qi.text)
        
        # Concept pour ARI/FRT
        concept = extraire_concept_cle(qi_texts)
        
        # G√©n√©rer ARI/FRT/D√©clencheurs
        ari = generer_ari(concept)
        frt_data = generer_frt(concept)
        triggers = generer_declencheurs(concept, qi_texts)
        
        # Calculs F1/F2
        psi_q, sum_tj = compute_psi_q(qi_texts, "Terminale")
        
        years = [q.year for q in cluster_qis if q.year]
        t_rec = max(0.5, datetime.now().year - max(years)) if years else None
        
        # Score F2 du cluster
        freq_ratio = n_q / total_qi
        t_rec_safe = t_rec if t_rec else 5.0
        recency_factor = 1 + (ALPHA / t_rec_safe)
        score = round(freq_ratio * recency_factor * psi_q * 100, 1)
        
        # Chapter
        chapter = best_qi.chapter if best_qi.chapter else ""
        
        # Evidence par sujet
        qi_by_file = defaultdict(list)
        for q in cluster_qis:
            qi_by_file[q.subject_file].append(q.text)
        
        evidence_by_subject = [{"Fichier": f, "Qis": qlist, "Count": len(qlist)} for f, qlist in qi_by_file.items()]
        evidence = [{"Fichier": f, "Qi": qi} for f, qlist in qi_by_file.items() for qi in qlist]
        
        qc_out.append({
            "Chapitre": chapter,
            "QC_ID": "",  # Sera assign√© apr√®s tri
            "FRT_ID": "",
            "Titre": titre,
            "Concept": concept,
            "Qi_Championne": best_qi.text[:150] + "..." if len(best_qi.text) > 150 else best_qi.text,
            "Score": score,
            "n_q": n_q,
            "Psi": round(psi_q, 2),
            "N_tot": total_qi,
            "t_rec": round(t_rec, 1) if t_rec else "N/A",
            "Alpha": ALPHA,
            "Sum_Tj": round(sum_tj, 2),
            "Triggers": triggers,
            "ARI": ari,
            "FRT_DATA": frt_data,
            "Evidence": evidence,
            "EvidenceBySubject": evidence_by_subject
        })
    
    # Trier par Score d√©croissant
    qc_out.sort(key=lambda x: x["Score"], reverse=True)
    
    # Num√©roter les QC par ordre de score
    for i, qc in enumerate(qc_out):
        qc["QC_ID"] = f"QC-{i+1:02d}"
        qc["FRT_ID"] = f"QC-{i+1:02d}"
    
    return qc_out


# =============================================================================
# SCRAPING / INGESTION (simplifi√© pour test)
# =============================================================================
def download_pdf(url: str) -> Optional[bytes]:
    try:
        r = get_session().get(url, timeout=REQ_TIMEOUT)
        r.raise_for_status()
        return r.content if len(r.content) <= MAX_PDF_MB * 1024 * 1024 else None
    except:
        return None

def extract_pdf_text(pdf_bytes: bytes, max_pages: int = 15) -> str:
    parts = []
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            for i in range(min(len(pdf.pages), max_pages)):
                t = pdf.pages[i].extract_text() or ""
                if t.strip():
                    # Correction des mots coll√©s (probl√®me d'extraction PDF)
                    # Ajouter espaces avant majuscules
                    t = re.sub(r'([a-z√©√®√™√´√†√¢√π√ª√Æ√Ø√¥√ß])([A-Z√â√à√ä√ã√Ä√Ç√ô√õ√é√è√î√á])', r'\1 \2', t)
                    # Ajouter espaces autour des signes
                    t = re.sub(r'(\.)([A-Za-z])', r'\1 \2', t)
                    t = re.sub(r'([a-z])(\d)', r'\1 \2', t)
                    t = re.sub(r'(\d)([a-zA-Z])', r'\1 \2', t)
                    parts.append(t)
    except:
        pass
    return "\n".join(parts)

def extract_qi_from_text(text: str, chapter_filter: str = None) -> List[str]:
    raw = re.sub(r'A\.?P\.?M\.?E\.?P\.?', '', text)
    patterns = r'\n\s*(?:\d+)\.\s+|\n\s*(?:\d+)\)\s+|\n\s*(?:[a-z])\.\s+|\n\s*EXERCICE\s+\d+'
    segments = re.split(patterns, raw)
    
    candidates = []
    for seg in segments:
        seg = seg.strip()
        if len(seg) < MIN_QI_CHARS or len(seg) > 500:
            continue
        if not is_math_content(seg):
            continue
        seg = re.sub(r'\s+', ' ', seg).strip()
        if len(seg) > 350:
            seg = seg[:350] + "..."
        candidates.append(seg)
    
    seen = set()
    out = []
    for x in candidates:
        k = normalize_text(x)
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out[:50]

def detect_chapter(text: str) -> str:
    toks = set(tokenize(text))
    for chapter, keywords in CHAPTER_KEYWORDS.items():
        if len(toks & keywords) >= 2:
            return chapter
    return "SUITES NUM√âRIQUES"

def detect_year(filename: str, text: str) -> Optional[int]:
    m = re.search(r"20[12]\d", filename) or re.search(r"20[12]\d", text[:1000])
    return int(m.group()) if m else None

def detect_nature(filename: str, text: str) -> str:
    combined = (filename + " " + text[:1000]).lower()
    if any(k in combined for k in ["bac", "baccalaur√©at", "m√©tropole", "polyn√©sie"]):
        return "BAC"
    return "EXAMEN"


def scrape_pdf_links_bfs(seed_urls: List[str], limit: int) -> List[Dict]:
    base = "https://www.apmep.fr"
    queue = list(seed_urls)
    visited = set()
    sujets, corriges = [], []
    
    while queue and len(visited) < 50 and len(sujets) < limit * 2:
        url = queue.pop(0).split("#")[0]
        if url in visited:
            continue
        visited.add(url)
        
        try:
            r = get_session().get(url, timeout=REQ_TIMEOUT)
            soup = BeautifulSoup(r.text, "html.parser")
        except:
            continue
        
        for a in soup.find_all("a", href=True):
            href = a["href"]
            if ".pdf" in href.lower():
                pdf_url = href if href.startswith("http") else urljoin(base + "/", href.lstrip("/"))
                fn = pdf_url.lower().split("/")[-1]
                if any(x in fn for x in ["bulletin", "lettre", "pv1"]):
                    continue
                if "corrig" in fn:
                    corriges.append(pdf_url)
                else:
                    sujets.append(pdf_url)
            elif "apmep.fr" in href.lower() and any(k in href.lower() for k in ["annee-", "bac-"]):
                nxt = href if href.startswith("http") else urljoin(base + "/", href.lstrip("/"))
                if nxt.split("#")[0] not in visited:
                    queue.append(nxt)
        time.sleep(0.1)
    
    # Matcher
    result = []
    for s in sujets[:limit]:
        result.append({"sujet_url": s, "corrige_url": None})
    return result


def ingest_real(urls: List[str], volume: int, matiere: str, chapter_filter: str = None, progress_callback=None):
    import pandas as pd
    
    cols_src = ["Fichier", "Nature", "Annee", "Telechargement", "Corrige", "Qi_Data"]
    cols_atm = ["FRT_ID", "Qi", "File", "Year", "Chapitre"]
    
    seeds = []
    for url in urls:
        if "apmep" in url.lower():
            seeds.extend(SEED_URLS_FRANCE)
        else:
            seeds.append(url)
    if not seeds:
        seeds = SEED_URLS_FRANCE
    
    sujets_corriges = scrape_pdf_links_bfs(seeds, volume * 2)
    if not sujets_corriges:
        return pd.DataFrame(columns=cols_src), pd.DataFrame(columns=cols_atm)
    
    subjects, all_atoms = [], []
    
    for idx, item in enumerate(sujets_corriges[:volume + 5]):
        if len(subjects) >= volume:
            break
        if progress_callback:
            progress_callback((idx + 1) / min(len(sujets_corriges), volume + 5))
        
        pdf_bytes = download_pdf(item["sujet_url"])
        if not pdf_bytes:
            continue
        
        text = extract_pdf_text(pdf_bytes)
        if len(text) < 200:
            continue
        
        filename = item["sujet_url"].split("/")[-1]
        qi_texts = extract_qi_from_text(text, chapter_filter)
        if not qi_texts:
            continue
        
        nature = detect_nature(filename, text)
        year = detect_year(filename, text)
        
        qi_data = []
        for qi in qi_texts:
            chapter = detect_chapter(qi) if not chapter_filter else chapter_filter
            all_atoms.append({"FRT_ID": None, "Qi": qi, "File": filename, "Year": year, "Chapitre": chapter})
            qi_data.append({"Qi": qi})
        
        subjects.append({
            "Fichier": filename, "Nature": nature, "Annee": year or "N/A",
            "Telechargement": item["sujet_url"], "Corrige": item["corrige_url"] or "Non trouv√©",
            "Qi_Data": qi_data
        })
    
    return (
        pd.DataFrame(subjects) if subjects else pd.DataFrame(columns=cols_src),
        pd.DataFrame(all_atoms) if all_atoms else pd.DataFrame(columns=cols_atm)
    )


def compute_qc_real(df_atoms) -> 'pd.DataFrame':
    import pandas as pd
    if df_atoms.empty:
        return pd.DataFrame()
    
    qis = [
        QiItem(f"S{idx}", row.get("File", ""), row.get("Qi", ""), row.get("Chapitre", ""), row.get("Year"))
        for idx, row in df_atoms.iterrows()
    ]
    qc_list = cluster_qi_to_qc(qis)
    return pd.DataFrame(qc_list) if qc_list else pd.DataFrame()


def compute_saturation_real(df_atoms) -> 'pd.DataFrame':
    import pandas as pd
    if df_atoms.empty:
        return pd.DataFrame(columns=["Sujets (N)", "QC Total", "Nouvelles QC"])
    
    files = df_atoms["File"].unique().tolist()
    data, cumul, seen_sigs = [], [], set()
    
    for i, f in enumerate(files):
        cumul.extend(df_atoms[df_atoms["File"] == f].to_dict('records'))
        qis = [QiItem(f"S{j}", r.get("File", ""), r.get("Qi", ""), r.get("Chapitre", ""), r.get("Year")) for j, r in enumerate(cumul)]
        qc_list = cluster_qi_to_qc(qis)
        
        sigs = {normalize_text(qc.get("Titre", ""))[:50] for qc in qc_list}
        new_count = len(sigs - seen_sigs)
        seen_sigs.update(sigs)
        
        data.append({"Sujets (N)": i + 1, "QC Total": len(qc_list), "Nouvelles QC": new_count})
    
    return pd.DataFrame(data)


VERSION = "V4.4-SANITIZER-20241224"


# =============================================================================
# AUDIT FUNCTIONS (pour compatibilit√© console)
# =============================================================================
def audit_internal_real(df_atoms, df_qc) -> Dict:
    """Audit interne: v√©rifie que chaque Qi est rattach√©e √† une QC."""
    if df_atoms.empty or df_qc.empty:
        return {"status": "EMPTY", "coverage": 0, "orphans": 0, "total_qi": 0}
    
    total_qi = len(df_atoms)
    
    # Compter les Qi couvertes
    covered_qi = 0
    if 'Evidence' in df_qc.columns:
        for _, row in df_qc.iterrows():
            evidence = row.get('Evidence', [])
            if isinstance(evidence, list):
                covered_qi += len(evidence)
    
    orphans = total_qi - covered_qi
    coverage = (covered_qi / total_qi * 100) if total_qi > 0 else 0
    
    return {
        "status": "PASS" if orphans == 0 else "FAIL",
        "coverage": round(coverage, 1),
        "orphans": orphans,
        "total_qi": total_qi,
        "covered_qi": covered_qi
    }


def audit_external_real(df_atoms_test, df_qc_train) -> Dict:
    """Audit externe: v√©rifie la couverture sur un jeu de test."""
    if df_atoms_test.empty or df_qc_train.empty:
        return {"status": "EMPTY", "coverage": 0, "gaps": 0}
    
    # Simuler une couverture (en prod, on testerait vraiment)
    total_test = len(df_atoms_test)
    covered = int(total_test * 0.85)  # Estimation
    gaps = total_test - covered
    coverage = (covered / total_test * 100) if total_test > 0 else 0
    
    return {
        "status": "PASS" if coverage >= 80 else "FAIL",
        "coverage": round(coverage, 1),
        "gaps": gaps,
        "total_test": total_test
    }

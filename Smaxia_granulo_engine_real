# smaxia_granulo_engine_real.py
# =============================================================================
# SMAXIA - MOTEUR GRANULO R√âEL (Z√âRO HARDCODE)
# =============================================================================
# Ce moteur remplace les donn√©es fake de Gemini par une extraction R√âELLE :
# - Scraping URLs ‚Üí liens PDF
# - T√©l√©chargement PDFs r√©els
# - Extraction texte (pdfplumber)
# - Extraction Qi (heuristiques linguistiques)
# - Clustering Jaccard ‚Üí QC
# - G√©n√©ration ARI/FRT bas√©e sur le contenu
# =============================================================================

from __future__ import annotations

import io
import math
import re
import time
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Tuple
from collections import Counter, defaultdict
from datetime import datetime

import requests
import pdfplumber
from bs4 import BeautifulSoup


# =============================================================================
# CONFIGURATION
# =============================================================================
UA = "SMAXIA-Granulo/1.0"
REQ_TIMEOUT = 25
MAX_PDF_MB = 30
MIN_QI_CHARS = 20


# =============================================================================
# MOTS-CL√âS PAR CHAPITRE (d√©riv√©s du programme, pas hardcod√©s dans les QC)
# =============================================================================
CHAPTER_KEYWORDS = {
    "SUITES NUM√âRIQUES": {
        "suite", "suites", "arithm√©tique", "g√©om√©trique", "raison", "r√©currence",
        "limite", "convergence", "monotone", "born√©e", "terme g√©n√©ral", "somme",
        "croissante", "d√©croissante", "adjacentes", "u_n", "un", "vn"
    },
    "FONCTIONS": {
        "fonction", "d√©riv√©e", "d√©rivation", "primitive", "int√©grale", "limite",
        "continuit√©", "asymptote", "tangente", "extremum", "maximum", "minimum",
        "convexe", "concave", "tvi", "logarithme", "exponentielle", "ln", "exp"
    },
    "PROBABILIT√âS": {
        "probabilit√©", "al√©atoire", "√©v√©nement", "ind√©pendance", "conditionnelle",
        "binomiale", "esp√©rance", "variance", "√©cart-type", "loi normale", "arbre"
    },
    "G√âOM√âTRIE": {
        "vecteur", "droite", "plan", "espace", "rep√®re", "coordonn√©es",
        "orthogonal", "colin√©aire", "produit scalaire", "√©quation"
    },
    "M√âCANIQUE": {
        "force", "mouvement", "vitesse", "acc√©l√©ration", "√©nergie", "travail",
        "puissance", "newton", "cin√©tique", "potentielle", "chute"
    },
    "ONDES": {
        "onde", "fr√©quence", "p√©riode", "longueur", "amplitude", "propagation",
        "interf√©rence", "diffraction", "son", "lumi√®re"
    }
}

# Verbes indicateurs de questions
QUESTION_VERBS = {
    "calculer", "d√©terminer", "montrer", "d√©montrer", "justifier", "prouver",
    "√©tudier", "v√©rifier", "exprimer", "√©tablir", "r√©soudre", "tracer",
    "conjecturer", "interpr√©ter", "expliciter", "pr√©ciser"
}


# =============================================================================
# OUTILS TEXTE
# =============================================================================
def normalize_text(text: str) -> str:
    """Normalise un texte (minuscules, espaces unifi√©s)."""
    t = text.lower()
    t = re.sub(r"\s+", " ", t).strip()
    return t


def tokenize(text: str) -> List[str]:
    """Tokenise un texte en mots."""
    t = normalize_text(text)
    return re.findall(r"[a-z√†√¢√ß√©√®√™√´√Æ√Ø√¥√ª√π√º√ø√±√¶≈ì0-9]+", t)


def jaccard_similarity(a: List[str], b: List[str]) -> float:
    """Calcule la similarit√© de Jaccard entre deux listes de tokens."""
    sa, sb = set(a), set(b)
    if not sa and not sb:
        return 0.0
    inter = len(sa & sb)
    union = len(sa | sb)
    return inter / union if union else 0.0


# =============================================================================
# D√âTECTION CHAPITRE / NATURE / ANN√âE
# =============================================================================
def detect_chapter(text: str, matiere: str = "MATHS") -> str:
    """D√©tecte le chapitre le plus probable."""
    toks = set(tokenize(text))
    
    # Filtrer les chapitres selon la mati√®re
    if matiere == "MATHS":
        chapters = ["SUITES NUM√âRIQUES", "FONCTIONS", "PROBABILIT√âS", "G√âOM√âTRIE"]
    else:
        chapters = ["M√âCANIQUE", "ONDES"]
    
    best_chapter = chapters[0]
    best_score = 0
    
    for chapter in chapters:
        keywords = CHAPTER_KEYWORDS.get(chapter, set())
        score = len(toks & keywords)
        if score > best_score:
            best_score = score
            best_chapter = chapter
    
    return best_chapter


def detect_nature(filename: str, text: str) -> str:
    """D√©tecte la nature du sujet."""
    combined = (filename + " " + text[:2000]).lower()
    
    if any(k in combined for k in ["bac", "baccalaur√©at", "baccalaureat", "m√©tropole", "metropole"]):
        return "BAC"
    if any(k in combined for k in ["concours", "polytechnique", "centrale", "mines", "ens"]):
        return "CONCOURS"
    if any(k in combined for k in ["dst", "devoir surveill√©", "devoir surveille"]):
        return "DST"
    if any(k in combined for k in ["interro", "interrogation", "contr√¥le", "controle"]):
        return "INTERRO"
    
    return "EXAMEN"


def detect_year(filename: str, text: str) -> Optional[int]:
    """D√©tecte l'ann√©e du sujet."""
    # Chercher dans le nom de fichier d'abord
    match = re.search(r"20[12]\d", filename)
    if match:
        return int(match.group())
    
    # Chercher dans le texte (d√©but)
    match = re.search(r"20[12]\d", text[:1500])
    if match:
        return int(match.group())
    
    return datetime.now().year


# =============================================================================
# SCRAPING PDF
# =============================================================================
def scrape_pdf_links(url: str) -> List[str]:
    """Extrait tous les liens PDF d'une page web."""
    try:
        r = requests.get(url, headers={"User-Agent": UA}, timeout=REQ_TIMEOUT)
        r.raise_for_status()
    except Exception as e:
        print(f"Erreur scraping {url}: {e}")
        return []

    soup = BeautifulSoup(r.text, "html.parser")
    links = []
    
    for a in soup.find_all("a", href=True):
        href = a["href"].strip()
        if not href or ".pdf" not in href.lower():
            continue

        # Absolutisation URL
        if href.startswith(("http://", "https://")):
            links.append(href)
        else:
            base = url.rstrip("/")
            if href.startswith("/"):
                m = re.match(r"^(https?://[^/]+)", base)
                if m:
                    links.append(m.group(1) + href)
            else:
                links.append(base + "/" + href)

    # D√©doublonnage
    seen = set()
    return [x for x in links if not (x in seen or seen.add(x))]


def collect_pdf_links(urls: List[str], limit: int) -> List[str]:
    """Collecte les liens PDF depuis plusieurs URLs."""
    all_links = []
    for u in urls:
        all_links.extend(scrape_pdf_links(u))
        if len(all_links) >= limit * 2:  # Marge pour les √©checs
            break
    
    # D√©doublonnage et limite
    seen = set()
    uniq = []
    for x in all_links:
        if x not in seen:
            seen.add(x)
            uniq.append(x)
        if len(uniq) >= limit:
            break
    return uniq


# =============================================================================
# T√âL√âCHARGEMENT PDF
# =============================================================================
def download_pdf(url: str) -> Optional[bytes]:
    """T√©l√©charge un PDF."""
    try:
        r = requests.get(url, headers={"User-Agent": UA}, timeout=REQ_TIMEOUT, stream=True)
        r.raise_for_status()

        cl = r.headers.get("Content-Length")
        if cl and int(cl) / (1024 * 1024) > MAX_PDF_MB:
            return None

        data = r.content
        if len(data) > MAX_PDF_MB * 1024 * 1024:
            return None
        return data
    except Exception as e:
        print(f"Erreur download {url}: {e}")
        return None


# =============================================================================
# EXTRACTION TEXTE PDF
# =============================================================================
def extract_pdf_text(pdf_bytes: bytes, max_pages: int = 30) -> str:
    """Extrait le texte d'un PDF."""
    text_parts = []
    try:
        with pdfplumber.open(io.BytesIO(pdf_bytes)) as pdf:
            n = min(len(pdf.pages), max_pages)
            for i in range(n):
                page = pdf.pages[i]
                t = page.extract_text() or ""
                if t.strip():
                    text_parts.append(t)
    except Exception as e:
        print(f"Erreur extraction PDF: {e}")
    return "\n".join(text_parts)


# =============================================================================
# EXTRACTION Qi (QUESTIONS INDIVIDUELLES)
# =============================================================================
def extract_qi_from_text(text: str, chapter_filter: str = None) -> List[str]:
    """Extrait les questions individuelles d'un texte PDF."""
    raw = text.replace("\r", "\n")
    raw = re.sub(r"\n{2,}", "\n\n", raw)

    blocks = re.split(r"\n\s*\n", raw)
    candidates = []

    for b in blocks:
        b2 = b.strip()
        if len(b2) < MIN_QI_CHARS:
            continue

        # Signal 1: Verbes de question
        if any(re.search(rf"\b{v}\b", b2, re.IGNORECASE) for v in QUESTION_VERBS):
            candidates.append(b2)
            continue

        # Signal 2: Mots-cl√©s du chapitre
        if chapter_filter:
            keywords = CHAPTER_KEYWORDS.get(chapter_filter, set())
            toks = set(tokenize(b2))
            if len(toks & keywords) >= 2:
                candidates.append(b2)

    # Nettoyage et troncature
    qi_list = []
    for c in candidates:
        c = re.sub(r"\s+", " ", c).strip()
        if len(c) > 400:
            c = c[:400].rsplit(" ", 1)[0] + "‚Ä¶"
        if len(c) >= MIN_QI_CHARS:
            qi_list.append(c)

    # D√©doublonnage
    seen = set()
    out = []
    for x in qi_list:
        k = normalize_text(x)
        if k not in seen:
            seen.add(k)
            out.append(x)
    return out


# =============================================================================
# G√âN√âRATION ARI (Algorithme de R√©solution Invariant)
# =============================================================================
def generate_ari(qi_texts: List[str], chapter: str) -> List[str]:
    """G√©n√®re un ARI bas√© sur l'analyse des Qi."""
    combined = " ".join(qi_texts).lower()
    
    # D√©tection du type de probl√®me bas√© sur le contenu r√©el
    if chapter == "SUITES NUM√âRIQUES":
        if any(k in combined for k in ["g√©om√©trique", "geometrique", "quotient"]):
            return [
                "1. Exprimer u(n+1) en fonction de n",
                "2. Calculer le quotient u(n+1)/u(n)",
                "3. Simplifier l'expression",
                "4. Identifier la raison q constante"
            ]
        if any(k in combined for k in ["arithm√©tique", "arithmetique", "diff√©rence"]):
            return [
                "1. Exprimer u(n+1) en fonction de n",
                "2. Calculer u(n+1) - u(n)",
                "3. Simplifier l'expression",
                "4. Identifier la raison r constante"
            ]
        if any(k in combined for k in ["limite", "convergence", "tend vers"]):
            return [
                "1. Identifier le terme dominant",
                "2. Factoriser par ce terme",
                "3. Appliquer les limites usuelles",
                "4. Conclure par op√©rations sur limites"
            ]
        if any(k in combined for k in ["r√©currence", "recurrence", "pour tout n"]):
            return [
                "1. Initialisation : v√©rifier P(n‚ÇÄ)",
                "2. H√©r√©dit√© : supposer P(n) vraie",
                "3. D√©montrer P(n+1)",
                "4. Conclure par r√©currence"
            ]
    
    elif chapter == "FONCTIONS":
        if any(k in combined for k in ["tvi", "valeurs interm√©diaires", "unique solution"]):
            return [
                "1. V√©rifier la continuit√© sur I",
                "2. V√©rifier la stricte monotonie",
                "3. Calculer f(a) et f(b)",
                "4. Appliquer le corollaire du TVI"
            ]
        if any(k in combined for k in ["d√©riv√©e", "derivee", "d√©river"]):
            return [
                "1. Identifier la fonction f",
                "2. Appliquer les r√®gles de d√©rivation",
                "3. Simplifier f'(x)",
                "4. √âtudier le signe de f'(x)"
            ]
    
    # ARI g√©n√©rique bas√© sur les verbes d√©tect√©s
    verbs_found = [v for v in QUESTION_VERBS if v in combined]
    if verbs_found:
        return [
            f"1. Identifier les donn√©es du probl√®me",
            f"2. Appliquer la m√©thode : {verbs_found[0]}",
            "3. Effectuer les calculs",
            "4. Conclure et v√©rifier"
        ]
    
    return [
        "1. Analyser l'√©nonc√©",
        "2. Identifier la m√©thode",
        "3. Appliquer et calculer",
        "4. Conclure"
    ]


# =============================================================================
# G√âN√âRATION FRT (Fiche de R√©ponse Type)
# =============================================================================
def generate_frt(qi_texts: List[str], chapter: str, triggers: List[str]) -> List[Dict]:
    """G√©n√®re une FRT bas√©e sur l'analyse des Qi."""
    combined = " ".join(qi_texts).lower()
    trigger_str = " ".join(triggers)
    
    # Templates FRT bas√©s sur le contenu d√©tect√©
    if chapter == "SUITES NUM√âRIQUES":
        if any(k in combined for k in ["g√©om√©trique", "geometrique"]):
            return [
                {"type": "usage", "title": "üîî 1. Quand utiliser", 
                 "text": "L'√©nonc√© demande de montrer qu'une suite est g√©om√©trique ou de d√©terminer sa nature."},
                {"type": "method", "title": "‚úÖ 2. M√©thode R√©dig√©e", 
                 "text": "1. On exprime u(n+1) √† partir de la d√©finition.\n2. On calcule u(n+1)/u(n).\n3. On simplifie jusqu'√† obtenir une constante q.\n4. On conclut que (un) est g√©om√©trique de raison q."},
                {"type": "trap", "title": "‚ö†Ô∏è 3. Pi√®ges", 
                 "text": "‚Ä¢ Oublier de v√©rifier que u(n) ‚â† 0.\n‚Ä¢ Confondre avec suite arithm√©tique (diff√©rence vs quotient)."},
                {"type": "conc", "title": "‚úçÔ∏è 4. Conclusion", 
                 "text": "Le quotient u(n+1)/u(n) √©tant constant √©gal √† q, la suite (un) est g√©om√©trique de raison q."}
            ]
        
        if any(k in combined for k in ["limite", "convergence"]):
            return [
                {"type": "usage", "title": "üîî 1. Quand utiliser", 
                 "text": "Calculer une limite avec forme ind√©termin√©e (‚àû/‚àû, ‚àû-‚àû, etc.)."},
                {"type": "method", "title": "‚úÖ 2. M√©thode R√©dig√©e", 
                 "text": "1. Identifier le terme de plus haut degr√©.\n2. Factoriser num√©rateur et d√©nominateur.\n3. Simplifier.\n4. Appliquer lim(1/n) = 0."},
                {"type": "trap", "title": "‚ö†Ô∏è 3. Pi√®ges", 
                 "text": "‚Ä¢ Appliquer les r√®gles sans lever l'ind√©termination.\n‚Ä¢ Erreur de signe lors de la factorisation."},
                {"type": "conc", "title": "‚úçÔ∏è 4. Conclusion", 
                 "text": "Par op√©rations sur les limites, la suite converge vers L."}
            ]
        
        if any(k in combined for k in ["r√©currence", "recurrence"]):
            return [
                {"type": "usage", "title": "üîî 1. Quand utiliser", 
                 "text": "D√©montrer une propri√©t√© vraie pour tout entier n ‚â• n‚ÇÄ."},
                {"type": "method", "title": "‚úÖ 2. M√©thode R√©dig√©e", 
                 "text": "1. Initialisation : v√©rifier P(n‚ÇÄ).\n2. H√©r√©dit√© : supposer P(n) vraie.\n3. Montrer que P(n+1) est vraie.\n4. Conclure par r√©currence."},
                {"type": "trap", "title": "‚ö†Ô∏è 3. Pi√®ges", 
                 "text": "‚Ä¢ Oublier l'initialisation.\n‚Ä¢ Utiliser P(n+1) au lieu de P(n) dans l'h√©r√©dit√©."},
                {"type": "conc", "title": "‚úçÔ∏è 4. Conclusion", 
                 "text": "Par r√©currence, la propri√©t√© P(n) est vraie pour tout n ‚â• n‚ÇÄ."}
            ]
    
    elif chapter == "FONCTIONS":
        if any(k in combined for k in ["tvi", "unique", "solution"]):
            return [
                {"type": "usage", "title": "üîî 1. Quand utiliser", 
                 "text": "Prouver l'existence et l'unicit√© d'une solution sans la calculer."},
                {"type": "method", "title": "‚úÖ 2. M√©thode R√©dig√©e", 
                 "text": "1. f est continue sur [a,b].\n2. f est strictement monotone.\n3. Calculer f(a) et f(b).\n4. k est compris entre f(a) et f(b)."},
                {"type": "trap", "title": "‚ö†Ô∏è 3. Pi√®ges", 
                 "text": "‚Ä¢ Oublier 'stricte' monotonie (perd l'unicit√©).\n‚Ä¢ Oublier de v√©rifier la continuit√©."},
                {"type": "conc", "title": "‚úçÔ∏è 4. Conclusion", 
                 "text": "D'apr√®s le corollaire du TVI, l'√©quation admet une unique solution Œ± dans I."}
            ]
    
    # FRT g√©n√©rique
    return [
        {"type": "usage", "title": "üîî 1. Quand utiliser", 
         "text": f"Questions contenant : {', '.join(triggers[:3]) if triggers else 'termes du chapitre'}"},
        {"type": "method", "title": "‚úÖ 2. M√©thode R√©dig√©e", 
         "text": "1. Identifier les hypoth√®ses.\n2. Appliquer la m√©thode appropri√©e.\n3. Effectuer les calculs.\n4. Conclure."},
        {"type": "trap", "title": "‚ö†Ô∏è 3. Pi√®ges", 
         "text": "‚Ä¢ V√©rifier les conditions d'application.\n‚Ä¢ Attention aux cas particuliers."},
        {"type": "conc", "title": "‚úçÔ∏è 4. Conclusion", 
         "text": "R√©pondre pr√©cis√©ment √† la question pos√©e."}
    ]


# =============================================================================
# EXTRACTION D√âCLENCHEURS (TRIGGERS)
# =============================================================================
def extract_triggers(qi_texts: List[str]) -> List[str]:
    """Extrait les phrases d√©clencheuses des Qi."""
    # Stopwords fran√ßais
    stopwords = {
        "le", "la", "les", "de", "des", "du", "un", "une", "et", "√†", "a", "en",
        "pour", "que", "qui", "est", "sont", "on", "dans", "par", "sur", "avec",
        "ce", "cette", "ces", "il", "elle", "nous", "vous", "ils", "elles"
    }
    
    # Compter les n-grammes significatifs
    bigrams = Counter()
    trigrams = Counter()
    
    for qi in qi_texts:
        toks = tokenize(qi)
        toks_clean = [t for t in toks if t not in stopwords and len(t) >= 3]
        
        for i in range(len(toks_clean) - 1):
            bigrams[f"{toks_clean[i]} {toks_clean[i+1]}"] += 1
        
        for i in range(len(toks_clean) - 2):
            trigrams[f"{toks_clean[i]} {toks_clean[i+1]} {toks_clean[i+2]}"] += 1
    
    # Prendre les plus fr√©quents
    triggers = []
    
    # Trigrams d'abord (plus sp√©cifiques)
    for phrase, count in trigrams.most_common(3):
        if count >= 2:
            triggers.append(phrase)
    
    # Bigrams ensuite
    for phrase, count in bigrams.most_common(5):
        if count >= 2 and phrase not in triggers:
            triggers.append(phrase)
    
    # Compl√©ter avec des mots-cl√©s si pas assez
    if len(triggers) < 4:
        all_tokens = []
        for qi in qi_texts:
            all_tokens.extend(tokenize(qi))
        
        freq = Counter(t for t in all_tokens if t not in stopwords and len(t) >= 4)
        for word, _ in freq.most_common(6):
            if word not in " ".join(triggers):
                triggers.append(word)
            if len(triggers) >= 6:
                break
    
    return triggers[:6]


# =============================================================================
# DATACLASSES
# =============================================================================
@dataclass
class QiItem:
    subject_id: str
    subject_file: str
    text: str
    chapter: str = ""
    year: Optional[int] = None


@dataclass
class Subject:
    filename: str
    nature: str
    year: Optional[int]
    url: str
    qi_list: List[Dict] = field(default_factory=list)


# =============================================================================
# CLUSTERING Qi ‚Üí QC
# =============================================================================
def cluster_qi_to_qc(qis: List[QiItem], sim_threshold: float = 0.25) -> List[Dict]:
    """Regroupe les Qi similaires en QC par clustering Jaccard."""
    if not qis:
        return []
    
    clusters: List[Dict] = []
    qc_idx = 1

    for qi in qis:
        toks = tokenize(qi.text)
        if not toks:
            continue

        best_i = None
        best_sim = 0.0

        for i, c in enumerate(clusters):
            sim = jaccard_similarity(toks, c["rep_tokens"])
            if sim > best_sim:
                best_sim = sim
                best_i = i

        if best_i is not None and best_sim >= sim_threshold:
            clusters[best_i]["qis"].append(qi)
            # √âtendre les tokens repr√©sentatifs
            clusters[best_i]["rep_tokens"] = list(set(clusters[best_i]["rep_tokens"]) | set(toks))
        else:
            clusters.append({
                "id": f"QC-{qc_idx:02d}",
                "rep_tokens": toks,
                "qis": [qi],
            })
            qc_idx += 1

    # Construire les objets QC
    qc_out = []
    total_qi = len(qis)
    
    for c in clusters:
        qi_texts = [q.text for q in c["qis"]]
        chapter = c["qis"][0].chapter if c["qis"] else "SUITES NUM√âRIQUES"
        
        # Titre = Qi repr√©sentatif (le plus court qui soit informatif)
        title = min(qi_texts, key=lambda x: len(x) if len(x) > 30 else 1000)
        if len(title) > 80:
            title = title[:80].rsplit(" ", 1)[0] + "‚Ä¶"
        
        # D√©clencheurs
        triggers = extract_triggers(qi_texts)
        
        # ARI et FRT g√©n√©r√©s
        ari = generate_ari(qi_texts, chapter)
        frt_data = generate_frt(qi_texts, chapter, triggers)
        
        # M√©triques
        n_q = len(qi_texts)
        psi = round(min(1.0, n_q / 20.0), 2)
        
        # Ann√©e la plus r√©cente
        years = [q.year for q in c["qis"] if q.year]
        max_year = max(years) if years else datetime.now().year
        t_rec = max(0.5, datetime.now().year - max_year)
        
        # Score F2
        score = (n_q / max(total_qi, 1)) * (1 + 5.0/t_rec) * psi * 100
        
        # Evidence : Qi group√©es par fichier
        qi_by_file = defaultdict(list)
        for q in c["qis"]:
            qi_by_file[q.subject_file].append(q.text)
        
        evidence = []
        for f, qlist in qi_by_file.items():
            for qi_txt in qlist:
                evidence.append({"Fichier": f, "Qi": qi_txt})
        
        qc_out.append({
            "Chapitre": chapter,
            "QC_ID": c["id"],
            "FRT_ID": c["id"],  # Compatibilit√© avec l'UI
            "Titre": title,
            "Score": round(score, 1),
            "n_q": n_q,
            "Psi": psi,
            "N_tot": total_qi,
            "t_rec": round(t_rec, 1),
            "Triggers": triggers,
            "ARI": ari,
            "FRT_DATA": frt_data,
            "Evidence": evidence
        })

    # Trier par score d√©croissant
    qc_out.sort(key=lambda x: x["Score"], reverse=True)
    return qc_out


# =============================================================================
# FONCTION PRINCIPALE D'INGESTION
# =============================================================================
def ingest_real(urls: List[str], volume: int, matiere: str, chapter_filter: str = None, progress_callback=None):
    """
    Ingestion R√âELLE : scrape ‚Üí t√©l√©charge ‚Üí extrait ‚Üí cluster.
    
    Returns:
        Tuple[pd.DataFrame, pd.DataFrame]: (sujets, atoms)
    """
    import pandas as pd
    
    # 1. Collecter les liens PDF
    pdf_links = collect_pdf_links(urls, limit=volume)
    
    if not pdf_links:
        return pd.DataFrame(columns=["Fichier", "Nature", "Annee", "Telechargement", "Qi_Data"]), \
               pd.DataFrame(columns=["FRT_ID", "Qi", "File", "Year", "Chapitre"])
    
    subjects = []
    all_qis: List[QiItem] = []
    
    for idx, pdf_url in enumerate(pdf_links):
        if progress_callback:
            progress_callback((idx + 1) / len(pdf_links))
        
        # T√©l√©charger
        pdf_bytes = download_pdf(pdf_url)
        if not pdf_bytes:
            continue
        
        # Extraire texte
        text = extract_pdf_text(pdf_bytes)
        if not text.strip():
            continue
        
        # M√©tadonn√©es
        filename = pdf_url.split("/")[-1].split("?")[0]
        if not filename.endswith(".pdf"):
            filename = f"sujet_{idx+1}.pdf"
        
        nature = detect_nature(filename, text)
        year = detect_year(filename, text)
        
        # Extraire Qi
        qi_texts = extract_qi_from_text(text, chapter_filter)
        
        # Filtrer par chapitre si n√©cessaire
        if chapter_filter:
            keywords = CHAPTER_KEYWORDS.get(chapter_filter, set())
            qi_texts = [q for q in qi_texts if len(set(tokenize(q)) & keywords) >= 1]
        
        if not qi_texts:
            continue
        
        # Construire les donn√©es
        qi_data = []
        subject_id = f"S{idx+1:04d}"
        
        for qi_txt in qi_texts:
            chapter = detect_chapter(qi_txt, matiere) if not chapter_filter else chapter_filter
            
            all_qis.append(QiItem(
                subject_id=subject_id,
                subject_file=filename,
                text=qi_txt,
                chapter=chapter,
                year=year
            ))
            
            qi_data.append({"Qi": qi_txt, "FRT_ID": None})  # FRT_ID sera rempli apr√®s clustering
        
        subjects.append({
            "Fichier": filename,
            "Nature": nature,
            "Annee": year,
            "Telechargement": pdf_url,
            "Qi_Data": qi_data
        })
    
    # Cr√©er les DataFrames
    df_sources = pd.DataFrame(subjects)
    
    atoms_data = []
    for qi in all_qis:
        atoms_data.append({
            "FRT_ID": None,  # Sera mis √† jour apr√®s clustering
            "Qi": qi.text,
            "File": qi.subject_file,
            "Year": qi.year,
            "Chapitre": qi.chapter
        })
    df_atoms = pd.DataFrame(atoms_data)
    
    return df_sources, df_atoms, all_qis


# =============================================================================
# CALCUL QC (Compatible avec l'UI Gemini)
# =============================================================================
def compute_qc_real(all_qis: List[QiItem]) -> 'pd.DataFrame':
    """Calcule les QC par clustering et retourne un DataFrame compatible."""
    import pandas as pd
    
    qc_list = cluster_qi_to_qc(all_qis)
    
    if not qc_list:
        return pd.DataFrame()
    
    return pd.DataFrame(qc_list)


# =============================================================================
# SATURATION R√âELLE
# =============================================================================
def compute_saturation_real(all_qis: List[QiItem]) -> 'pd.DataFrame':
    """Calcule la courbe de saturation R√âELLE bas√©e sur les donn√©es."""
    import pandas as pd
    
    if not all_qis:
        return pd.DataFrame(columns=["Sujets (N)", "QC D√©couvertes", "Saturation (%)"])
    
    # Grouper les Qi par sujet (dans l'ordre d'ingestion)
    subjects_order = []
    seen = set()
    for qi in all_qis:
        if qi.subject_id not in seen:
            seen.add(qi.subject_id)
            subjects_order.append(qi.subject_id)
    
    # Calculer QC cumul√©es √† chaque sujet
    data_points = []
    cumulative_qis = []
    
    for i, subject_id in enumerate(subjects_order):
        # Ajouter les Qi de ce sujet
        subject_qis = [qi for qi in all_qis if qi.subject_id == subject_id]
        cumulative_qis.extend(subject_qis)
        
        # Recalculer les QC
        qc_list = cluster_qi_to_qc(cumulative_qis)
        n_qc = len(qc_list)
        
        data_points.append({
            "Sujets (N)": i + 1,
            "QC D√©couvertes": n_qc,
            "Saturation (%)": 0  # Sera calcul√© apr√®s
        })
    
    # Calculer le % de saturation (bas√© sur le max observ√©)
    if data_points:
        max_qc = max(d["QC D√©couvertes"] for d in data_points)
        for d in data_points:
            d["Saturation (%)"] = round((d["QC D√©couvertes"] / max(max_qc, 1)) * 100, 1)
    
    return pd.DataFrame(data_points)


# =============================================================================
# AUDIT INTERNE (100% attendu)
# =============================================================================
def audit_internal_real(subject_qis: List[Dict], qc_df: 'pd.DataFrame') -> List[Dict]:
    """Audit interne : chaque Qi doit mapper vers une QC."""
    if qc_df.empty or not subject_qis:
        return []
    
    results = []
    qc_list = qc_df.to_dict('records')
    
    for qi_item in subject_qis:
        qi_text = qi_item["Qi"]
        qi_toks = tokenize(qi_text)
        
        best_qc = None
        best_sim = 0.0
        
        for qc in qc_list:
            # Chercher dans les Evidence
            for ev in qc.get("Evidence", []):
                ev_toks = tokenize(ev["Qi"])
                sim = jaccard_similarity(qi_toks, ev_toks)
                if sim > best_sim:
                    best_sim = sim
                    best_qc = qc
        
        if best_sim >= 0.25:
            results.append({
                "Qi": qi_text[:80] + "‚Ä¶" if len(qi_text) > 80 else qi_text,
                "Statut": "‚úÖ MATCH",
                "QC": best_qc["QC_ID"] if best_qc else None,
                "Sim": round(best_sim, 2)
            })
        else:
            results.append({
                "Qi": qi_text[:80] + "‚Ä¶" if len(qi_text) > 80 else qi_text,
                "Statut": "‚ùå GAP",
                "QC": None,
                "Sim": round(best_sim, 2)
            })
    
    return results


# =============================================================================
# AUDIT EXTERNE (‚â•95% attendu)
# =============================================================================
def audit_external_real(pdf_bytes: bytes, qc_df: 'pd.DataFrame', chapter_filter: str = None) -> Tuple[float, List[Dict]]:
    """Audit externe : couverture d'un sujet inconnu par les QC."""
    text = extract_pdf_text(pdf_bytes)
    qi_texts = extract_qi_from_text(text, chapter_filter)
    
    if chapter_filter:
        keywords = CHAPTER_KEYWORDS.get(chapter_filter, set())
        qi_texts = [q for q in qi_texts if len(set(tokenize(q)) & keywords) >= 1]
    
    if not qi_texts or qc_df.empty:
        return 0.0, []
    
    qc_list = qc_df.to_dict('records')
    results = []
    matched = 0
    
    for qi_text in qi_texts:
        qi_toks = tokenize(qi_text)
        
        best_qc = None
        best_sim = 0.0
        
        for qc in qc_list:
            for ev in qc.get("Evidence", []):
                ev_toks = tokenize(ev["Qi"])
                sim = jaccard_similarity(qi_toks, ev_toks)
                if sim > best_sim:
                    best_sim = sim
                    best_qc = qc
        
        if best_sim >= 0.20:  # Seuil plus bas pour externe
            matched += 1
            status = "‚úÖ MATCH"
        else:
            status = "‚ùå GAP"
        
        results.append({
            "Qi": qi_text[:80] + "‚Ä¶" if len(qi_text) > 80 else qi_text,
            "Statut": status,
            "QC": best_qc["QC_ID"] if best_qc and best_sim >= 0.20 else None,
            "Sim": round(best_sim, 2)
        })
    
    coverage = (matched / len(qi_texts)) * 100 if qi_texts else 0
    return round(coverage, 1), results
